import json

import pandas

from themis import ANSWER, ANSWER_ID, TITLE, FILENAME, QUESTION, CONFIDENCE, IN_PURVIEW, CORRECT
from themis import logger, CsvFileType, pretty_print_json
from themis.question import QUESTION_TEXT, TOP_ANSWER_TEXT

QUESTION_TEXT_INPUT = "QuestionText"  # Column header for input file required by Annotation Assist
QUESTION_TEXT_OUTPUT = "Question_Text"  # Columns header for output file created by Annotation Assist
IS_IN_PURVIEW = "Is_In_Purview"
SYSTEM_ANSWER = "System_Answer"
ANNOTATION_SCORE = "Annotation_Score"
TOP_ANSWER_TEXT_ANNOTATION_ASSIST = "TopAnswerText"
TOP_ANSWER_CONFIDENCE = "TopAnswerConfidence"
ANS_LONG = "ANS_LONG"
ANS_SHORT = "ANS_SHORT"
IS_ON_TOPIC = "IS_ON_TOPIC"


def annotation_assist_qa_input(answers, questions, judgments):
    """
    Create list of Q&A pairs for judgment by Annotation Assist.

    The Q&A pairs to be judged are compiled from sets of answers generated by Q&A systems. These may be filtered by an
    optional list of questions. Judgements may be taken from optional sets of previously judged Q&A pairs.

    :param answers: answers to questions as generated by Q&A systems
    :type answers: pandas.DataFrame
    :param questions: optional set of questions to filter on, if None use all answered questions
    :type questions: pandas.DataFrame
    :param judgments: optional judgments, look up a judgment here before sending the Q&A pair to Annotation Assist
    :type judgments: pandas.DataFrame
    :return: Q&A pairs to pass to Annotation Assist for judgment
    :rtype: pandas.DataFrame
    """
    qa_pairs = pandas.concat(answers)
    qa_pairs = qa_pairs.drop_duplicates([QUESTION, ANSWER])
    logger.info("%d Q&A pairs" % len(qa_pairs))
    if questions is not None:
        qa_pairs = pandas.merge(qa_pairs, questions)
        logger.info("%d Q&A pairs for %d unique questions" % (len(qa_pairs), len(questions)))
    if judgments:
        judged_qa_pairs = pandas.concat(judgments)
        assert not any(judged_qa_pairs.duplicated()), "There are Q&A pairs with multiple judgements"
        qa_pairs = pandas.merge(qa_pairs, judged_qa_pairs, on=(QUESTION, ANSWER), how="left")
        not_judged = qa_pairs[qa_pairs[CORRECT].isnull()]
        n = len(not_judged)
        logger.info("%d unjudged Q&A pairs (%0.3f%%)" % (n, 100.0 * n / len(qa_pairs)))
    else:
        not_judged = qa_pairs
    not_judged = not_judged.rename(
        columns={QUESTION: QUESTION_TEXT_INPUT, ANSWER: TOP_ANSWER_TEXT_ANNOTATION_ASSIST,
                 CONFIDENCE: TOP_ANSWER_CONFIDENCE})
    not_judged = not_judged[[QUESTION_TEXT_INPUT, TOP_ANSWER_TEXT_ANNOTATION_ASSIST, TOP_ANSWER_CONFIDENCE]]
    return not_judged


def create_annotation_assist_corpus(corpus):
    """
    Create the JSON corpus file used by the Annotation Assist tool.

    :param corpus: corpus generated by 'xmgr corpus' command
    :type corpus: pandas.DataFrame
    :return: JSON representation of the corpus used by Annotation Assist
    :rtype: str
    """
    corpus["splitPauTitle"] = corpus[TITLE].apply(lambda title: title.split(":"))
    corpus = corpus.rename(columns={ANSWER: "text", ANSWER_ID: "pauId", TITLE: "title", FILENAME: "fileName"})
    return pretty_print_json(json.loads(corpus.to_json(orient="records"), encoding="utf-8"))


def interpret_annotation_assist(annotation_assist, judgment_threshold):
    """
    Convert the file produced by the Annotation Assist tool into a set of judgments that can be used by Themis.

    Convert the in purview column from an integer value to a boolean. Convert the annotation score column to a boolean
    correct column by applying a threshold. An answer can only be correct if the question is in purview. Drop any Q&A
    pairs that have multiple annotations.

    :param annotation_assist: Annotation Assist judgments
    :type annotation_assist: pandas.DataFrame
    :param judgment_threshold: threshold above which an answer is deemed correct
    :type judgment_threshold: pandas.DataFrame
    :return: Annotation Assist judgments with a boolean Correct column
    :rtype: pandas.DataFrame
    """
    qa_duplicates = annotation_assist[[QUESTION, ANSWER]].duplicated()
    if any(qa_duplicates):
        n = sum(qa_duplicates)
        logger.warning(
            "Dropping %d Q&A pairs with multiple annotations (%0.3f%%)" % (n, 100.0 * n / len(annotation_assist)))
        annotation_assist.drop_duplicates((QUESTION, ANSWER), keep=False, inplace=True)
    annotation_assist[IN_PURVIEW] = annotation_assist[IN_PURVIEW].astype("bool")
    annotation_assist[CORRECT] = \
        annotation_assist[IN_PURVIEW] & (annotation_assist[ANNOTATION_SCORE] >= judgment_threshold)
    logger.info("Processed %d judgments" % len(annotation_assist))
    return annotation_assist.drop(ANNOTATION_SCORE, axis="columns")


class AnnotationAssistFileType(CsvFileType):
    """
    Read the file produced by the `Annotation Assist <https://github.com/cognitive-catalyst/annotation-assist>` tool.
    """

    def __init__(self):
        super(self.__class__, self).__init__([QUESTION_TEXT_OUTPUT, IS_IN_PURVIEW, SYSTEM_ANSWER, ANNOTATION_SCORE],
                                             {QUESTION_TEXT_OUTPUT: QUESTION, IS_IN_PURVIEW: IN_PURVIEW,
                                              SYSTEM_ANSWER: ANSWER})


class JudgmentFileType(CsvFileType):
    """
    Read the file produced by the 'judge interpret' command.
    """

    def __init__(self):
        super(self.__class__, self).__init__([QUESTION, ANSWER, IN_PURVIEW, CORRECT])

    @staticmethod
    def output_format(judgments):
        judgments = judgments.sort_values([QUESTION, ANSWER])
        return judgments.set_index([QUESTION, ANSWER])


def augment_usage_log(usage_log, judgments):
    """
    Add In Purview and Annotation Score information to system usage log.

    :param usage_log: user interaction logs from QuestionsData.csv XMGR report
    :type usage_log: pandas.DataFrame
    :param judgments: judgments
    :type judgments: pandas.DataFrame
    :return: user interaction logs with additional columns
    :rtype: pandas.DataFrame
    """
    usage_log = usage_log.rename(columns={QUESTION_TEXT: QUESTION, TOP_ANSWER_TEXT: ANSWER})
    augmented = pandas.merge(usage_log, judgments, on=(QUESTION, ANSWER), how="left")
    n = len(usage_log[[QUESTION, ANSWER]].drop_duplicates())
    if n:
        m = len(judgments)
        logger.info("%d unique question/answer pairs, %d judgments (%0.3f%%)" % (n, m, 100.0 * m / n))
    return augmented.rename(columns={QUESTION: QUESTION_TEXT, ANSWER: TOP_ANSWER_TEXT})
